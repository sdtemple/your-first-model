{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95625a28-2a7c-44d1-afce-3b67c0d06100",
   "metadata": {},
   "source": [
    "# Train Vision Transformer for a single audio time slice\n",
    "\n",
    "We make raw code blocks for more user functionality. Convert these to code blocks, thereby making custom models outside the scope of the command line interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5374ffe-2903-40a9-8576-e01702aab94f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201bd082-3b90-424c-a2a2-c011d218efae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 20:40:53.814038: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-13 20:40:54.223720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# array data manipulation and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from transformers import ViTConfig, ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "# machine learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, \\\n",
    "    accuracy_score, \\\n",
    "    f1_score, \\\n",
    "    auc, \\\n",
    "    recall_score, \\\n",
    "    precision_score, \\\n",
    "    precision_recall_curve, \\\n",
    "    roc_curve\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd870080-0cfc-4601-9b11-7b38885b5f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73dcb697-ff66-4397-bfdf-aebd8e02be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for single-channel image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # Resize to pretrained ViT size\n",
    "    transforms.ToTensor(),          # Converts to [0,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0f0a95f-f3c8-4b4c-8c01-0f73109d02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**4\n",
    "X_train_path = os.path.join('data', 'X_train.npy')\n",
    "X_test_path = os.path.join('data', 'X_test.npy')\n",
    "Y_train_path = os.path.join('data', 'Y_train.npy')\n",
    "Y_test_path = os.path.join('data', 'Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa8ac7c1-16dc-4d9d-8882-295f688c41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD DATA ---\n",
    "X_train = np.load(X_train_path)\n",
    "X_test = np.load(X_test_path)\n",
    "Y_train = np.load(Y_train_path)\n",
    "Y_test = np.load(Y_test_path)\n",
    "num_classes = len(np.unique(Y_train))\n",
    "\n",
    "# --- MAKE TENSORS ---\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Apply to a single image (H, W) or (C, H, W)\n",
    "X_test = torch.stack([transform(img) for img in X_test])  # batch dimension\n",
    "X_train = torch.stack([transform(img) for img in X_train])  # batch dimension\n",
    "\n",
    "# --- DATA LOADER ---\n",
    "train_data = TensorDataset(X_train, Y_train)\n",
    "test_data = TensorDataset(X_test, Y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# X_height = X_train.shape[1]\n",
    "# X_width = X_train.shape[2]\n",
    "\n",
    "del X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46a24abc-1c6f-43ae-b700-d11977cd715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "hidden_size = 6**3\n",
    "num_hidden_layers = 6\n",
    "num_attention_heads = 6\n",
    "intermediate_size = 1024\n",
    "hidden_dropout_prob = 0.1\n",
    "attention_probs_dropout_prob = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865ea08-d1b7-4191-8bb0-4a78e9f24ab9",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e942f8e-66a6-4d9b-909d-92f72bcf5a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 216, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=216, out_features=216, bias=True)\n",
       "              (key): Linear(in_features=216, out_features=216, bias=True)\n",
       "              (value): Linear(in_features=216, out_features=216, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=216, out_features=216, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=216, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=216, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((216,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((216,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((216,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=216, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ViTConfig(\n",
    "    patch_size=32,\n",
    "    num_channels=1,  # Adjust for your input data (e.g., 1 for grayscale, 3 for RGB)\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    intermediate_size=intermediate_size,\n",
    "    hidden_dropout_prob=hidden_dropout_prob,\n",
    "    attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "    num_labels=num_classes,\n",
    ")\n",
    "\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# criterion choice\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# choice of optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)   \n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd0f3c-436d-4a9c-bd5d-3a6dc726dcf7",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbec8ad3-cf17-4eb6-831b-5abcadcb9b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.2452\n",
      "Epoch 2/100, Loss: 2.2438\n",
      "Epoch 3/100, Loss: 2.2509\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Pass to GPU\u001b[39;00m\n\u001b[32m     16\u001b[39m     inputs = inputs.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/turbo/umor-sethtem/hoodriver/lib/python3.12/site-packages/torch/_compile.py:51\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/turbo/umor-sethtem/hoodriver/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/turbo/umor-sethtem/hoodriver/lib/python3.12/site-packages/torch/optim/optimizer.py:967\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m         p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    969\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass to GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26efffed-ef55-4d17-95e7-19c33bc984cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21484375\n",
      "0.16796875\n",
      "0.19140625\n",
      "0.18359375\n",
      "0.2265625\n",
      "0.21484375\n",
      "0.24609375\n",
      "0.21484375\n",
      "0.2578125\n",
      "0.19140625\n",
      "0.16796875\n",
      "0.171875\n",
      "0.18359375\n",
      "0.1953125\n",
      "0.1875\n",
      "0.19921875\n",
      "0.203125\n",
      "0.24609375\n",
      "0.16796875\n",
      "0.21875\n",
      "0.16015625\n",
      "0.21484375\n",
      "0.18359375\n",
      "0.234375\n",
      "0.15625\n",
      "0.1640625\n",
      "0.2109375\n",
      "0.20703125\n",
      "0.1796875\n",
      "0.21875\n",
      "0.23828125\n",
      "0.2109375\n",
      "0.20703125\n",
      "0.1953125\n",
      "0.234375\n",
      "0.1484375\n",
      "0.22265625\n",
      "0.2265625\n",
      "0.22265625\n",
      "0.13671875\n",
      "0.1640625\n",
      "0.1484375\n",
      "0.171875\n",
      "0.1640625\n",
      "0.1796875\n",
      "0.18359375\n",
      "0.21875\n",
      "0.17578125\n",
      "0.20703125\n",
      "0.234375\n",
      "0.21484375\n",
      "0.19140625\n",
      "0.16796875\n",
      "0.1640625\n",
      "0.16796875\n",
      "0.1953125\n",
      "0.2109375\n",
      "0.17578125\n",
      "0.15625\n",
      "0.203125\n",
      "0.16796875\n",
      "0.1953125\n",
      "0.15625\n",
      "0.1953125\n",
      "0.14453125\n",
      "0.16796875\n",
      "0.2109375\n",
      "0.1875\n",
      "0.1796875\n",
      "0.234375\n",
      "0.17578125\n",
      "0.2265625\n",
      "0.18359375\n",
      "0.171875\n",
      "0.23828125\n",
      "0.18359375\n",
      "0.20703125\n",
      "0.1640625\n",
      "0.1875\n",
      "0.1796875\n",
      "0.265625\n",
      "0.15625\n",
      "0.20703125\n",
      "0.23046875\n",
      "0.19921875\n",
      "0.25\n",
      "0.16796875\n",
      "0.1875\n",
      "0.171875\n",
      "0.2109375\n",
      "0.21484375\n",
      "0.23046875\n",
      "0.2109375\n",
      "0.18359375\n",
      "0.2109375\n",
      "0.1640625\n",
      "0.18359375\n",
      "0.1640625\n",
      "0.1640625\n",
      "0.1796875\n",
      "0.1640625\n",
      "0.19140625\n",
      "0.15555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy on the validation data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class (index of the maximum value in the output)\n",
    "        # For multi-class (one-hot) targets\n",
    "        predicted = outputs.logits.argmax(dim=1)\n",
    "        labels = labels.int()\n",
    "        predicted = predicted.cpu()\n",
    "        labels = labels.cpu()\n",
    "        print(accuracy_score(predicted,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc4bd8cc-fb7f-4295-8823-5ddf010ee35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05859375\n",
      "0.03515625\n",
      "0.0\n",
      "0.58984375\n",
      "0.484375\n",
      "0.453125\n",
      "0.22265625\n",
      "0.0\n",
      "0.0\n",
      "0.1015625\n",
      "0.08984375\n",
      "0.03515625\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy on the validation data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class (index of the maximum value in the output)\n",
    "        # For multi-class (one-hot) targets\n",
    "        predicted = outputs.logits.argmax(dim=1)\n",
    "        labels = labels.int()\n",
    "        predicted = predicted.cpu()\n",
    "        labels = labels.cpu()\n",
    "        print(accuracy_score(predicted,labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hood River",
   "language": "python",
   "name": "hoodriver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
